from event.io.dataset.base import (
    Span,
    DataLoader,
    DEDocument,
    Corpus,
)
import json
import os
import sys
from event.arguments.prepare.event_vocab import EmbbedingVocab

from traitlets.config import Configurable
from traitlets import (
    Int,
    List,
    Unicode,
)
from event.util import load_mixed_configs, set_basic_log

import logging
import numpy as np
from collections import defaultdict
import pprint
from scipy.spatial.distance import cosine
from event.mention import aida_maps
import traceback


class ZeroShotEventResources(Configurable):
    """Resource class."""

    event_embedding_path = Unicode(help="Event Embedding path").tag(config=True)
    word_embedding_path = Unicode(help="Word Embedding path").tag(config=True)

    event_vocab_path = Unicode(help="Event Vocab").tag(config=True)
    word_vocab_path = Unicode(help="Word Vocab").tag(config=True)

    target_ontology = Unicode(help="Ontology path").tag(config=True)

    def __init__(self, **kwargs):
        super(ZeroShotEventResources, self).__init__(**kwargs)

        self.event_embedding = np.load(self.event_embedding_path)
        self.word_embedding = np.load(self.word_embedding_path)

        self.event_embed_vocab = EmbbedingVocab(self.event_vocab_path)
        self.word_embed_vocab = EmbbedingVocab(self.word_vocab_path)

        with open(self.target_ontology) as onto_file:
            self.onto_set = set()

            self.ontology = json.load(onto_file)

            for frame in self.ontology["frames"]:
                self.onto_set.add(frame["@id"])

        logging.info(f"{len(self.event_embed_vocab.vocab)} events in embedding.")

        logging.info(f"{len(self.word_embed_vocab.vocab)} words in embedding.")


def camel_slash_split(s, lower=True):
    l_s = [[]]

    for c in s:
        if c.isupper():
            l_s.append([])

        if c == "_":
            l_s.append([])
        else:
            l_s[-1].append(c)

    if lower:
        return ["".join(l).lower() for l in l_s if len(l) > 0]
    else:
        return ["".join(l) for l in l_s]


def event_type_split(s):
    if s in aida_maps.kbp_type_split_map:
        return aida_maps.kbp_type_split_map[s]
    else:
        return camel_slash_split(s)


class ZeroShotTypeMapper:
    def __init__(self, resources):
        self.resources = resources
        # A map from the ontology event type to its tokens
        self.onto_event_tokens = {}
        self.onto_arg_role_tokens = {}

        self.onto_arg_domain = defaultdict(list)

        # From a fine-grained to a parent
        self.type_parent = {}
        # The embedding of the predicates
        self.pred_embeds = {}

        self.tokenize_ontology()

    def tokenize_ontology(self):
        for frame in self.resources.ontology["frames"]:
            onto_category = frame["@type"]
            if onto_category == "event_type":
                event_type = frame["@id"]
                self.onto_event_tokens[event_type] = {}
                level_types = event_type.split(":")[1].split(".")

                tokenized_types = []
                for lt in level_types:
                    tokenized_types.append([])
                    for t in camel_slash_split(lt):
                        if t not in aida_maps.ldc_ontology_skips:
                            t = aida_maps.onto_token_nom_map.get(t, t)
                            pred_id = self.resources.event_embed_vocab.get_index(
                                t + "-pred", None
                            )
                            if pred_id >= 0:
                                tokenized_types[-1].append(t)
                                pred_vector = self.resources.event_embedding[pred_id]
                                self.pred_embeds[t] = pred_vector
                            else:
                                logging.debug(f"Predicate form for {t} not " f"found")

                self.onto_event_tokens[event_type]["top"] = tokenized_types[0]
                if len(level_types) > 2:
                    self.onto_event_tokens[event_type]["middle"] = tokenized_types[1]
                    self.onto_event_tokens[event_type]["low"] = tokenized_types[2]

                    if not frame["subClassOf"] == "aida:EventType":
                        self.type_parent[event_type] = frame["subClassOf"]

            if onto_category == "event_argument_role_type":
                role_type = frame["@id"]
                role_token = role_type.split("_")[-1]
                self.onto_arg_role_tokens[role_type] = [role_token.lower()]
                self.onto_arg_domain[frame["domain"]].append(role_type)

    def frame_lemma_direct(self, frame, lemma):
        return aida_maps.frame_lemma_map.get((frame, lemma), None)

    def head_token_direct(self, lemma):
        if lemma in aida_maps.token_direct_map:
            return aida_maps.token_direct_map[lemma]

    def arg_direct(self, content, entities):
        for arg in content["arguments"]:
            arg_lemma = entities[arg["entityId"]]["lemma"]
            if arg_lemma in aida_maps.arg_direct_map:
                return aida_maps.arg_direct_map[arg_lemma][0]

    def map_from_event_type(self, event_type, lemma):
        # print("mapping tac kbp event type ", event_type, lemma)

        level1, level2 = event_type.split("_")
        level2_tokens = event_type_split(level2)

        l_score, m_score, full_type = self.map_by_pred_match(
            [t + "-pred" for t in level2_tokens], [lemma + "-pred"]
        )

        if m_score > 0.8 or l_score > 0.8:
            if l_score > 0.8:
                return full_type
            else:
                if full_type in self.type_parent:
                    # print('parent is ', self.type_parent[full_type])
                    return self.type_parent[full_type]
                else:
                    # print('no parent')
                    return full_type

    def map_from_lemma_only(self, lemma):
        l_score, m_score, full_type = self.map_by_pred_match(
            [lemma + "-pred"], [lemma + "-pred"]
        )

        if m_score > 0.8 or l_score > 0.8:
            if l_score > 0.8:
                return full_type
            else:
                if full_type in self.type_parent:
                    return self.type_parent[full_type]
                else:
                    return full_type

    def map_from_frame(self, frame, lemma):
        if frame in aida_maps.frame_direct_map:
            return aida_maps.frame_direct_map[frame]

        lemma_pred = lemma + "_pred"
        # print(frame, lemma_pred)
        l_score, m_score, full_type = self.map_by_pred_match(
            [frame], [frame, lemma_pred]
        )

        if m_score > 0.7 or l_score > 0.7:
            if l_score > 0.7:
                return full_type
            else:
                if full_type in self.type_parent:
                    return self.type_parent[full_type]
                else:
                    return full_type

    def map_by_pred_match(self, middle_matchers, low_matchers):
        scored_pairs = []

        for onto_type, onto_type_tokens in self.onto_event_tokens.items():
            middle_score = 0
            low_score = 0

            mid_tokens = onto_type_tokens.get("middle", []) + onto_type_tokens.get(
                "top", []
            )

            for onto_t in mid_tokens:
                if onto_t in self.pred_embeds:
                    onto_emd = self.pred_embeds[onto_t]
                    for t in middle_matchers:
                        t_id = self.resources.event_embed_vocab.get_index(t, None)
                        if t_id >= 0:
                            t_emd = self.resources.event_embedding[t_id]
                            s = 1 - cosine(onto_emd, t_emd)

                            if s > middle_score:
                                middle_score = s

            low_tokens = onto_type_tokens.get("low", [])
            for onto_t in low_tokens:
                if onto_t in self.pred_embeds:
                    onto_emd = self.pred_embeds[onto_t]
                    for lemma_pred in low_matchers:
                        lemma_idx = self.resources.event_embed_vocab.get_index(
                            lemma_pred, None
                        )

                        if lemma_idx >= 0:
                            emd = self.resources.event_embedding[lemma_idx]
                            s = 1 - cosine(onto_emd, emd)
                            if s > low_score:
                                low_score = s

            rank_num = max(low_score, middle_score)

            scored_pairs.append((rank_num, low_score, middle_score, onto_type))

        scored_pairs.sort(reverse=True)

        for rank_num, low_score, mid_score, t in scored_pairs:
            if mid_score < 0.2:
                continue
            return low_score, mid_score, t

        return 0, 0, None

    def map_event_type(self, event, entities):
        event_head = event["headLemma"]

        head_direct_type = self.head_token_direct(event_head)
        if head_direct_type:
            return "head_direct", head_direct_type

        if event["component"] == "CrfMentionTypeAnnotator":
            t = event["type"]
            if (t, event.get("frame", "NA")) in aida_maps.kbp_frame_correction:
                t = aida_maps.kbp_frame_correction[t, event["frame"]]

            if (t, event_head) in aida_maps.kbp_lemma_map:
                t = aida_maps.kbp_lemma_map[(t, event_head)]
                return "map_kbp_lemma", t

            if t in aida_maps.kbp_direct_map:
                return "map_kbp_direct", aida_maps.kbp_direct_map[t]

        arg_direct_type = self.arg_direct(event, entities)
        if arg_direct_type:
            return "arg_direct", arg_direct_type

        if "frame" in event:
            t = self.frame_lemma_direct(event["frame"], event["headLemma"])
            if t:
                return "map_from_frame", t

            t = self.map_from_frame(event["frame"], event["headLemma"])
            if t:
                return "map_from_frame", t

        if event["component"] == "CrfMentionTypeAnnotator":
            # Mapping from event map is less reliable.
            t = event["type"]
            t = self.map_from_event_type(t, event["headLemma"])
            if t:
                return "map_from_event_type", t

            if t in aida_maps.kbp_backup_map:
                return "map_kbp_backup", aida_maps.kbp_backup_map[t]

        if event["component"] == "VerbBasedEventDetector":
            t = self.map_from_lemma_only(event["headLemma"])
            if t:
                return "map_from_head_lemma", t

    def map_arg_role(self, evm, arg, entities):
        arg_lemma = entities[arg["entityId"]]["lemma"]
        event_type = evm["type"]
        event_head = evm["headLemma"]

        # List the event types in a hierarchy, with the specific one first. In
        # such cases, we will look for the low ontology items first.
        l_types = [event_type]
        t_parts = event_type.split(".")
        if len(t_parts) > 2:
            l_types.append(".".join(t_parts[:2]))
        if len(t_parts) > 1:
            l_types.append(".".join(t_parts[:1]))
        l_types.append(t_parts[0])

        # List the roles, with frame element first.
        # We trust fn more, but "other" the least, so they are at the end.
        l_roles = []

        other_roles = []
        for role in arg["roles"]:
            prefix, r = role.split(":", 1)
            if prefix == "fn":
                l_roles.insert(0, r)
            elif prefix == "other":
                other_roles.append(r)
            else:
                l_roles.append(r)

        l_roles.extend(other_roles)

        if arg_lemma in aida_maps.arg_direct_map:
            if event_type == aida_maps.arg_direct_map[arg_lemma][0]:
                return aida_maps.arg_direct_map[arg_lemma][1]
        else:
            for role in l_roles:
                # Go through the event type hierarchy, then go up.
                for t in l_types:
                    if role in aida_maps.srl_ldc_arg_map.get(t, {}):
                        return event_type + "_" + aida_maps.srl_ldc_arg_map[t][role]
                    if role == "ARGM-LOC" or role == "Place":
                        return f"{event_type}_Place"
                else:
                    if role == "ARGM-TMP" or role == "Time":
                        continue
                    debug_file.write(
                        f"frame: {evm.get('frame', 'no_frame')} , "
                        f"head: {event_head} , "
                        f"arg: {arg_lemma} , "
                        f"role: {role} , "
                        f"event type: {event_type}\n"
                    )


def process_one(type_mapper, resources, fin, fout):
    rich_doc = json.load(fin)
    # text = rich_doc['text']

    entities = {}
    for ent in rich_doc["entityMentions"]:
        entities[ent["id"]] = {
            "span": ent["span"],
            "head_span": ent["headWord"]["span"],
            "text": ent["text"],
            "lemma": ent["headWord"]["lemma"],
            "id": ent["id"],
        }

    for evm in rich_doc["eventMentions"]:
        # print('evm is **' + evm['headLemma'] + '**')
        map_res = type_mapper.map_event_type(evm, entities)
        if not map_res:
            # print('No mapping results')
            continue

        # print('Mapping results is ', map_res)
        # input('take a look')

        rule, mapped_type = map_res

        if mapped_type and mapped_type in resources.onto_set:
            evm["type"] = mapped_type

            for arg in evm["arguments"]:
                roles = []
                mapped_role = type_mapper.map_arg_role(evm, arg, entities)

                if mapped_role:
                    if mapped_role in resources.onto_set:
                        roles.append(mapped_role)
                    else:
                        debug_file.write(f"Mapped role not valid: " f"{mapped_role}\n")

                roles.extend(arg["roles"])
                arg["roles"] = roles

    json.dump(rich_doc, fout)


def main(para, resources):
    type_mapper = ZeroShotTypeMapper(resources)

    if not os.path.exists(para.output_path):
        os.makedirs(para.output_path)

    for p in os.listdir(para.input_path):
        if not p.endswith(".json"):
            continue
        with open(os.path.join(para.input_path, p)) as fin, open(
            os.path.join(para.output_path, p), "w"
        ) as fout:
            try:
                process_one(type_mapper, resources, fin, fout)
            except Exception as err:
                sys.stderr.write(
                    f"ERROR: Exception in ZeroShotPredictor while " f"processing p\n"
                )
                traceback.print_exc()
                logging.error(traceback.format_exc())


if __name__ == "__main__":

    class Basic(Configurable):
        input_path = Unicode(help="Input path.").tag(config=True)
        output_path = Unicode(help="Output path.").tag(config=True)

    debug_file = open("zero_shot_event_debug.txt", "w")

    set_basic_log()
    conf = load_mixed_configs()
    basic_para = Basic(config=conf)
    res = ZeroShotEventResources(config=conf)

    main(basic_para, res)

    debug_file.close()
